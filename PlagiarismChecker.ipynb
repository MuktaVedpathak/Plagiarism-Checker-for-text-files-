{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68287b8-e66a-44ee-b430-4c13832b3610",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "52626057-75c5-4fa9-a87f-0085310c06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90b971-72b6-42da-b91d-bd53a7d7cd53",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df97e79f-0657-431c-ab01-9d8829e97433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = text.strip() # Remove whitespace\n",
    "    tokens = word_tokenize(text) # Tokenize text (Breaks text in small peices or tokens)\n",
    "    stop_words = set(stopwords.words('english')) # Remove stopwords (removes a,an,the,in, on,at,and,but type of words)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens) # Rejoin tokens into a single string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fc308-5994-474a-80c9-0fa1fdca7053",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da281a95-fdcd-4f94-b3bd-2bc65f933058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(directory):\n",
    "    files = [doc for doc in os.listdir(directory) if doc.endswith('.txt')]\n",
    "    contents = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            processed_content = preprocess_text(content)\n",
    "            contents.append(processed_content)\n",
    "    return files, contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda1c9c-569b-48ec-b801-62aeb3adf4ef",
   "metadata": {},
   "source": [
    "# Computing cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5fb04cd-0469-4bd3-93ca-57419ef12fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(contents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(contents)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    similarity_matrix = np.round(similarity_matrix * 100, 2) #Convert similarity score to percentage and round off to 2 digits\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a15a82a0-c6e2-457b-b095-0cbe49f2f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(files, similarity_matrix):\n",
    "    results = []\n",
    "    num_files = len(files)\n",
    "    for i in range(num_files):\n",
    "        for j in range(i + 1, num_files):\n",
    "            score = similarity_matrix[i, j]\n",
    "            results.append((files[i], files[j], score))\n",
    "    return pd.DataFrame(results, columns=['Document A', 'Document B', 'Similarity Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca01b6-5424-4488-b3ff-5d5591a0ce87",
   "metadata": {},
   "source": [
    "# Checking similarity score for all text files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1307696e-4987-49fa-99f4-c7fb95e2bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Document A      Document B  Similarity Score\n",
      "0    doc1.txt        doc2.txt             42.16\n",
      "1    doc1.txt        doc3.txt             34.02\n",
      "2    doc1.txt        doc4.txt             33.38\n",
      "3    doc1.txt        doc5.txt             85.80\n",
      "4    doc1.txt  What's new.txt              5.90\n",
      "5    doc2.txt        doc3.txt             18.41\n",
      "6    doc2.txt        doc4.txt             17.96\n",
      "7    doc2.txt        doc5.txt             36.17\n",
      "8    doc2.txt  What's new.txt              4.11\n",
      "9    doc3.txt        doc4.txt              9.56\n",
      "10   doc3.txt        doc5.txt             35.72\n",
      "11   doc3.txt  What's new.txt              5.43\n",
      "12   doc4.txt        doc5.txt             28.64\n",
      "13   doc4.txt  What's new.txt              1.93\n",
      "14   doc5.txt  What's new.txt              7.12\n"
     ]
    }
   ],
   "source": [
    "def main(directory):\n",
    "    files, contents = load_files(directory)\n",
    "    similarity_matrix = compute_similarity(contents)\n",
    "    results_df = generate_results(files, similarity_matrix)\n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = 'C:\\\\Mukta\\\\PlagarismChecker_project'  # Replace with your directory\n",
    "    results = main(directory)\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
